# AI-in-Production-Gen-AI-and-Agentic-AI-at-Scale-Notes
This Repository contains my Udemy course notes of "AI in Production: Gen AI and Agentic AI at Scale" by Ed Donner

**Table of Contents:**

**I) Week 1**

**A) Day 1 - Instant AI Deployment: Your First Production App on Vercel in Minutes**

**B) Day 1 - From Zero to Live: Deploying Your First AI-Powered SaaS on Vercel**

**C) Day 1 - From AI Concepts to Cloud Deployment: Navigating the DevOps Landscape**

**D) Day 1 - Course Overview: Building Production AI Systems Across 4 Weeks**

**E) Day 1 - Deploy Your First Live AI App with OpenAI and Vercel Integration**

**F) Day 1 - Managing API Costs and Environment Setup for Production AI Systems**

**G) Day 1 - Course Expectations and Community Support for Production AI**

**H) Day 2 - Building Full-Stack AI Apps: Frontend-Backend Architecture for LLMs**

**I) Day 2 - Building Full-Stack AI Apps with React, FastAPI, and NextJS**

**J) Day 2 - Building Your First Full-Stack AI SaaS with NextJS and FastAPI**

**K) Day 2 - Building Your First FastAPI Backend for Production LLM Deployment**

**L) Day 2 - Deploying Full-Stack AI Apps with Next.js Frontend and FastAPI Backend**

**M) Day 2 - Adding Real-Time Streaming and Professional UI to Your LLM App**



# **A) Day 1 - Instant AI Deployment: Your First Production App on Vercel in Minutes**

This is a big moment. This is the start of the juiciest course I've ever made. Welcome to the beginning of generative AI and Agentic AI in production, week one, day one. The next four weeks are about taking AI and putting it into production, making it production grade.

But wait, before we get started, I must stop you right there. It is super important in these kinds of courses to begin with objectives, introduction, curriculum, and logistics to set you up for success. But no, people have taken my courses before. Know that that is not my style. It's not my jam. We will not start with objectives or introduction or curriculum or logistics. We will start with instant gratification by getting straight into it. Let's go ahead.

Before we do any talking, let's deploy something to production right now. And so let's get to it. Instant gratification. We're going to begin in a web page, in a GitHub web page at this site: github.com. I'm going to put this link wherever I possibly can so you find it easy to find. It should be in the Udemy resources. If you can't find it, then you might have to type it in, but it will be there somewhere. Look for it: Editor Donna Production. That's the repo in GitHub. Go into the folder called "week one" and then look at the file called "day one instant gratification."

Let me tell you two things right away. First, what you see when you do this might look different from what I've got in the videos, and that's because I constantly update this as people encounter any snafus doing this. I will keep it up to date and with the latest information. So what you see might be richer than what I show in the video. It will only be improved, so don't worry if it looks a bit different. Also, as we go through these instructions, when you do it, if you have a slightly different experience on a different website, then don't worry about it. It's possible that in different regions, some of these websites look a bit different. It's also possible that they've changed over time. See if it fits. Easy to figure out what you're supposed to do. Just read the cues from the screen, follow along, and if not, message me in Udemy and we'll figure it out together.

All right, with that in mind, let's get to instant gratification. We are going to make a production deployment in a matter of minutes. Step one is to sign up for something called Vercel. You may be wondering what Vercel is. You'll see it in just a second. Follow this link here to Vercel. Vercel allows you to build and deploy on the cloud. It's an incredibly easy-to-use cloud infrastructure service that lets you build and scale AI apps and deploy them really, really quickly.

The first thing we're going to do is click "Sign Up." It says, "Your first deploy is just a sign up away." I'm going to set it to personal projects. I give myself a name, say continue, and then you get an authentication screen where you can choose to authenticate using Google, GitHub, GitLab, or Bitbucket. Pick your poison, go through, sign up for an account, and then follow the rest of the instructions that are on the GitHub page. You should now have your own Vercel account.

Next step is to install the IDE that we will use, which is Cursor. For those who were on my Agentic course, you're already familiar with this. For others, here are the instructions to install Cursor. If you'd rather use a different IDE, that's totally fine; any IDE will do. If you use something like VSCode, Cursor is itself a fork of VSCode, as is Windsurf, so it will be very familiar. Visit Cursor and press the download button. For Mac, download for Mac OS; for Windows, download for Windows. Follow the installation wizard, selecting defaults. Once installed, open up Cursor in a terminal and create a new project by clicking "New," and we will create one called "Instant."

When you open up Cursor, it might look like this, or you might have to go to File → New Window. Press the "Open Project" button. In the dialog, create your project somewhere—it's normal to have a folder called "projects" within your home directory. Create a new folder and call it "instant." Open it, and now you have a new fresh project ready for work. Close the chat screen in Cursor; we're focusing on the Instant Project and following the instructions in GitHub.

Step three is to create a FastAPI application. FastAPI is a Python server that acts as an app server. It can respond to different web requests by running Python code. In this case, it's a simple server responding to the slash route, returning the text "live from production." Copy the code from GitHub, create a new file called instant.py in Cursor, paste the code, and save the file. Remember, saving is crucial; it's not like a Google doc.

Next, create a requirements.txt file with the listed contents: FastAPI and uvicorn. This file lists Python package dependencies. Save it. Then, create a file called vercel.json where we define the Vercel project configuration. Paste in the contents and save it. The JSON has sections on builds and routes, indicating that any incoming requests will be handled by instant.py.

Step six: install Node. If you don’t have it installed, download it from the official Node website, stick with defaults unless you know otherwise, and install it. Open a terminal in Cursor (Ctrl + backtick on Windows or Command + backtick on Mac) and check Node installation using node --version and npm --version.

Next, install Vercel using npm. While it's installing, don't worry too much about warnings—they happen all the time. Then run vercel login to log in with the method you used to sign up. Finally, the moment of truth: run vercel . while in the project folder. Follow the prompts: select scope, create a new project (name it "instant" or anything you like), confirm the code directory, and skip additional settings. Vercel creates necessary files and deploys the project.

Once deployed, you'll see information about your deployment including a URL. Click the URL (Command + click on Mac, Control + click on Windows) to open it in a browser. You should see "live from production," confirming that you have deployed to production online. You could be forgiven for thinking this isn't yet an AI app per se; it’s just the first step. This is the foundation for deploying your first production AI app. We'll build on this in the next video.

# **B) Day 1 - From Zero to Live: Deploying Your First AI-Powered SaaS on Vercel**

Now, I appreciate that some of you have seen Vercel before and deployed your own projects and are probably thinking, “Ah, this is small potatoes.” Maybe it is, maybe not yet mind-blowing. But hopefully, for those new to it, you were pleased to see that it’s really not that hard.

If we go back to Cursor, in addition to the link to production, there was another link called "Inspect." I want to take you through that command. Click the link, and it shows us the screen in Vercel where we can look at our deployment, seeing a little preview right there. One thing you might have noticed is that when you went to this web page, you actually had to be signed in to Vercel to access it. You could argue that it’s not really live on the internet if you need to be signed into Vercel, but we can change that. If you expand the deployment configuration section, you’ll see deployment protection is turned on. Clicking that takes us to the settings page, where Vercel authentication ensures visitors need to be logged in and a member of our team. Unfortunately, only we could see this right now, but luckily, we can turn it off, press save, and it confirms the change. Now, for example, copy this URL to a new incognito window in Chrome, and you'll see "live from production" being served to anyone on the internet. There we are—live on the internet in minutes. Never fear, we’ll be coming back to add a healthy dose of artificial intelligence soon.

This is what I have in store for you. We are going to do some hardcore learning. This is probably the most intensive course I’ve ever made. It’s all in four weeks, but I have packed it full. We’ll work on some of the juiciest projects I’ve ever built—very commercial, very hands-on, really interesting, and quite challenging. At the same time, I hope to make it fun. That’s my vibe: making sure it’s entertaining as well as educational. Hopefully, you’ll enjoy building things that are scalable, resilient, and secure.

Let me tell you about the objectives, who the course is for, and how you’ll know you’ve been successful at the end. There are really two main audiences for this course. First, entrepreneurs. If you want to take a SaaS app and deploy it online on the internet—with authentication, SSL, subscriptions, and a secure, scalable setup—then this course is for you. The first week is the most intense because we’ll do all of that and then build on it. Second, if you want enterprise-level knowledge on AI deployed in the cloud for scale and resiliency—working with major cloud providers like AWS, GCP, and Azure, and gaining relevant expertise to apply for roles that require this—then this course is also for you.

That said, there’s a continuum of people between these two extremes. Maybe you want to be an entrepreneur but currently work at a company. Maybe you’re in a medium-sized company and want to deploy for future growth. Maybe you want to beef up your resume to apply for more roles. If you’re anywhere on that spectrum, this course is designed for you.

So, what will you get out of it at the end? What are the success factors? There are three main outcomes. First, for the entrepreneur, you should be able to deploy, monetize, and make money from your own AI application. Of course, you’ll need product-market fit—I can’t promise people will pay for it—but the technology side will be solved. Second, you’ll be able to implement production-grade AI at major companies, gaining transferable skills that you can apply in different situations. I focus on transferable skills and common platforms and frameworks so you’ll have that capability. Third, you’ll be able to apply for roles that require this skill set. I’ve reviewed many job descriptions and, as a hiring manager myself, know what’s needed. Other courses cover a lot, but one piece often missing is AI productionization—and that’s exactly what you’ll gain from this course.

That said, the course alone won’t do it. You’ll need to complete the assignments as we go. The best way to learn is by doing. Listening to me isn’t enough; you need to actively engage. When you get stuck, that’s when the real learning happens. You need to complete the projects with me, fix the problems when they arise, make the projects your own, add your own flair, and take on the assignments I give you. That’s the real way to learn. The videos and instructions are just the foundation—the part that matters is what you do.

# **C) Day 1 - From AI Concepts to Cloud Deployment: Navigating the DevOps Landscape**

And now let me introduce myself to convince you that I’m actually qualified to talk to you about this. With apologies to those who have heard my intro about three times before, it’s going to be exactly the same and make the same jokes—just put me on two-times speed if you like. For those new to me, I’m Ed Donner. I am the co-founder and CTO of an AI startup called Nebula. Go check us out. Most of my career was at JP Morgan, where I ended up as an MD leading a team of about 300 technologists. I started out in London, which is where I’m from originally, as you can probably tell from my accent. I spent a few great months in Tokyo, and then ended up in NYC, where I live now.

Before Nebula, I founded an AI startup called Untapped, which was acquired a few years ago. There’s a picture from Times Square showing our acquisition announcement—a magical moment for me. If you squint carefully, you can even see a few pixels of me in the image. A bit of a leap of faith, but that is indeed me on the Times Square billboard at that moment. The other reason I show this photo is not just to boast, but also because I actually live about a block from Times Square, behind that guitar you see in the picture. While I’m in London right now, when I’m in New York, you can find me right there looking out at you.

It’s customary with these things to show a personal picture or a hobby. Here’s a picture of me in front of a plane I just flew. You might think I’m skilled at this, but quite the contrary. My skills with LMS are only surpassed by my complete inability to do anything requiring hand-eye coordination. If you ever see me in a cockpit ready to control a plane, you’ll want to look for a parachute quickly. However, if you’re here to take a course on building and deploying LMS, generative AI, and AI in production, then luckily, that is exactly my skill set. Let’s get on with the course.

You might be wondering how this course fits with my other courses. My first course was LLM Engineering, which teaches the foundations of AI engineering—how to select and apply LLMs, how to do things like RAG and fine-tuning, and covers the foundations of generative AI. Shortly after that, I made a companion course that goes deeper into the field of generative AI. These courses are designed as companions: you can take them in either order, one, or both. The AI in Production course sits on top of these two. You don’t need to have taken them, though prior knowledge on RAG and fine-tuning will help you follow along more smoothly. In this course, we won’t be coding things like RAG; instead, we’ll use pre-built components for data ingestion and storage in vector databases, and I’ll guide you through it.

I also have another course called the AI Briefing for Founders and Leaders. This is my only course that isn’t technical—it’s commercial and intended for people who want to understand how to build durable, valuable AI-driven businesses. Some participants in this course may benefit from it, particularly if they’re looking to commercialize LLMs. All my courses are complementary, with minimal overlap, and can be taken in any order. A natural progression might be LLM Engineering for foundations, Generative AI for application, and then AI in Production for deployment.

I aim for my courses to appeal to a wide range of backgrounds. For some, parts may be too fast or complex, in which case take it slow and explore the material yourself. For others, parts may be too simple—deploying to Vercel may be familiar—so you can speed through those sections. I want to set your expectations: this course is highly requested, and many people may have misconceptions about it. When people think of deploying AI software to production, they might expect a course heavily focused on AI concerns. While AI is certainly part of it, 70–80% of this course is actually about platform engineering—DevOps, configuring cloud providers, and production deployment. Whether the Python code is a simple "hello from production" or calling an LLM, the bulk of your work now is infrastructure, not AI per se.

Throughout this course, we will maintain an AI lens. We will work with vector databases and APIs like Bedrock and SageMaker, but much of the work is traditional cloud platform engineering. We’ll cover observability, deploying MCP servers, and other relevant practices, but platform engineering is the majority of what you’ll learn. I have structured this course to follow a T-shaped learning approach: covering breadth across cloud providers, CI/CD, GitHub Actions, and going deep in one area—AWS—since it is the most common in industry. While we’ll touch on Google Cloud and Azure, the focus is AWS to best prepare you for industry standards, and I’ll highlight parallels between providers as we go.

# **D) Day 1 - Course Overview: Building Production AI Systems Across 4 Weeks**

And now I'm excited to unveil our four-week curriculum to become production grade. As usual with my courses, I've organized this into four different modules, one for each week. The first week is focused on building a SaaS project that you deploy onto the internet, including features like authentication and subscription.

In the second week, we’re going to focus on AWS, drilling down on platform engineering for an AI project deployed on AWS. We’ll cover a wide range of AWS concerns, leaving you able to deploy your own projects and at least familiar with the components we don’t touch. This will include infrastructure as code and CI/CD.

The third week will involve a variety of activities related to AI projects. We’ll first take a look at Azure and GCP so that you’ve explored all the major providers. We’ll focus on data engineering by building ingest pipelines, and we’ll examine vector stores and MCP so that you can deploy an agent with MCP servers behind it.

In the fourth week, we turn to Agentic AI. We’ll build and deploy a multi-agent system in production, which will lead to the capstone project and the conclusion of the four weeks. Each of these four modules represents a week of learning together on this journey, with five days in each week representing the different components of the course.

In the first week, which we’re currently in, we’ll build up an app with a front end and back end, implement authentication and subscriptions, and create a healthcare SaaS app. This app will first be deployed to Vercel, and then we’ll perform our first deployment to AWS. In the second week, we’ll discuss AWS architecture, revisit a project from one of my previous courses called The Digital Twin, and create a new version of it. We’ll use Bedrock, add Terraform for infrastructure as code, and GitHub Actions to establish a robust production deployment pipeline.

Week three is a mixed bag of activities. We’ll start a new project, the Cyber Security Analyst, and deploy it on Azure and GCP using MCP servers. We’ll then use AWS SageMaker and build data ingest pipelines using S3, vectors, and MCP servers. The final week, “Gigantic AI in Production,” focuses on multi-agent deployments. We’ll work on the capstone project, including the front end, and integrate observability, monitoring, and security—three crucial aspects of AI and Agentic AI deployments. We’ll also explore Agentic AI platforms, which are extremely relevant and hot in the industry right now.

The capstone project we’ll work on is designed to be a commercial-ready product, with opportunities for you to add your own elements and potentially monetize it. It will be an agentic financial planner SaaS product, allowing users to enter details about accounts, investments, equity portfolios, and retirement accounts. The system will provide advice on rebalancing portfolios and retirement prospects. This fully functioning Agentic app will have different agents working in concert to add value and be useful for the user community, while allowing you to customize and deploy it.

You may be wondering about the different colored tiles in the curriculum overview. The yellow tiles represent DevOps and platform engineering concerns. Blue tiles denote projects, including the healthcare app, Digital Twin Mark II, and the capstone project. The Cyber Security project is small and doesn’t get a blue square, but it’s still counted among the four projects. Purple squares indicate AI-related content. Even though today we haven’t yet focused on AI, the day is young. Remember, only 20% of the course is AI, but there are plenty of purple squares throughout. Even AI components like Bedrock and SageMaker involve significant DevOps work, but the purple tiles confirm the AI focus.

By the end of these four weeks, having completed your projects and assignments, you’ll be well-deserving of the recognition and cup at the end. More importantly, you’ll have acquired the skills to independently deploy commercial LM products, Gemini, and Agentic AI into production. This course has a fairly comprehensive scope, though production deployment is an enormous field, so some areas are beyond our focus. We primarily work on AWS while touching on other providers. Our focus is on transferable, industry-standard skills. For example, for infrastructure as code, we’ll use Terraform rather than Amazon CDK, as it applies across AWS, GCP, and Azure. Similarly, for authentication, we use Clerk, which is provider-agnostic.

We’ll touch on front-end development and Docker containers, but only at a surface level. Entire courses exist on front-end development and Docker, and we won’t go deep here, though self-study guides will be provided. You’ll still have the code available, but for any significant changes, large language models can assist. The goal is to provide you the tools to apply principles learned here to other components, providers, and APIs. After repeating these practices across multiple projects, you’ll develop the ability to research and integrate services independently.

In summary, our goal is to cover a focused subset of production deployment concepts while enabling you to generalize these skills to other contexts. I’ve been talking long enough—now it’s time to do some hands-on work. Let’s go back and add some AI spice to our simple production application.

# **E) Day 1 - Deploy Your First Live AI App with OpenAI and Vercel Integration**

So here I am back on GitHub at github.com, and we’re navigating to week one, specifically week one/day one.part 2.md, which is our Instant Gratification Project Part Two. The first step is to set up an OpenAI API key, as we’ll be making calls to GPT-5. I realize that some of you may already have OpenAI keys, in which case you can skip step one. Others may not have a key or may not want one, since OpenAI requires a $5 upfront payment, which you spend as you go. While the cost is minimal—a fraction of a cent per call—it can still be a consideration for those using multiple APIs. There are free and cheap alternatives that don’t require this upfront payment, and these are carefully noted in the guides folder in the documentation. The guides explain how you can modify the instructions if you choose one of these alternatives.

Assuming you decide to proceed with OpenAI, it’s a good experience to understand how to set up a key and use it, as it’s very common in industry. To get started, visit platforms.openai.com to sign up for an account. If you’re unsure about the difference between ChatGPT the product and using the OpenAI API, this is also covered in the docs. Once signed up, you may need to create an organization, authenticate using Google or other methods, and add your $5 minimum payment. Make sure auto-recharge is turned off to avoid unintended charges. Next, create a new API key by selecting Create secret new secret key, which will usually start with src-proj-. Copy this key to your clipboard and store it safely, ideally in a password manager. If you save it in a text editor, use a plain editor rather than something like Microsoft Word, as word processors may alter characters like dashes.

Once you have your OpenAI key, it’s time to add it to Vercel. The instructions indicate running the command "vercel env add OpenAI_API_key" in Cursor. When prompted, paste in your key, making sure it starts with src-proj- and has no trailing spaces. Next, select which environment the key applies to; for this course, choose all environments by pressing A when prompted or by using space and arrow keys. If a mistake is made, you can always repeat this process. At this point, we’ve successfully set environment variables in Vercel.

The next step is updating our dependencies. In requirements.txt, where we’ve already listed Python libraries like fastapi and uvicorn, we now add "openai" to include the OpenAI Python client library. It’s important to note that this library does not contain any LLM code; it’s an open-source utility provided by OpenAI that constructs HTTP requests and converts responses into Python objects. This wrapper allows us to interact with OpenAI models or even other LLM providers. My guide explains how to use alternative providers while still using this same library.

Now we move on to updating our application code. From GitHub, copy the new code and paste it into instant.py in Cursor, replacing the existing code. This code uses FastAPI and defines a single route that will be called when someone visits it. It creates a new instance of OpenAI using the Python utility, constructs a message prompt: "You're on a website that's just been deployed to production for the first time. Please reply with an enthusiastic announcement to welcome visitors to the site, explaining that it's live on production for the first time." You can customize this message as desired. This message is wrapped in a list of dictionaries with "role": "user" and "content" set to the message.

The response is generated with "client.completions.create" using the lightweight GPT-5 nano model, passing in the messages. The returned content is accessed with "response.choices[0].message.content". Any carriage returns in the response are replaced with an HTML line break tag <br/>. We then construct an HTML string for the webpage with a title and body containing the AI-generated reply. Always save the page before proceeding.

Finally, deploy the project with "vercel deploy". The build process runs, and shortly we’ll have our first AI project live on the internet. Opening the preview shows a generated announcement, like: "Ha. Welcome to our brand new site with a lovely emoji. Now live in production for the very first time, we're thrilled to announce that our production deployment is live! This milestone means real users, real data, and our real working product." GPT-5 has provided an enthusiastic introduction, which may be formal, snarky, or playful depending on your input. Although Vercel calls this a preview, the project is live and accessible online. To safeguard your $5 credit, you can restrict access via Vercel authentication, though with it off, anyone can interact with your generative AI deployment.

# **F) Day 1 - Managing API Costs and Environment Setup for Production AI Systems**

It’s almost a wrap for our first day together on this journey, and I have two more important messages for you. I know you may be a little tired of messages, but these are crucial, so please pay attention. The first message is about API costs, which is extremely important. You are in complete control of your API spending and have full freedom to decide whether to spend nothing or some amount. At the same time, you are responsible for managing these costs. Most of the things we do in this course are free, and I will highlight them as we go. Some activities, particularly those involving AWS, may incur small charges. For example, AWS Lambda offers a generous free tier, so most of what we do there won’t cost anything. Overall, for the entire course, the expenses should remain around $5, maybe up to $10 if you choose to go beyond the basics. The cost could increase if you decide to register your own domains, but this is optional and entirely your choice.

Regarding the $5–$10 range I mentioned, this is not guaranteed, and it’s important to understand that most major cloud providers, such as AWS, GCP, and Azure, do not operate the same way OpenAI does. OpenAI allows a simple pay-as-you-go approach, but enterprise cloud providers require a credit card and accumulate charges without a hard cap. While you can set alerts and quotas, there is no automatic stop, so monitoring your spending is your responsibility. You can always choose not to spend anything, and even when optional costs arise, you can skip them and still gain valuable learning. API costs must always be monitored carefully, and I will show you how to track them and set alerts. Unlike OpenAI, mainstream cloud platforms rarely offer hard caps, so vigilance is essential.

The second message is a lighter, though equally important, one about the challenges of environment setup. In my other courses, the first few days involve installing software, configuring keys, setting up APIs, and similar activities. These setup tasks make up less than 20% of the course time, but they generate about 80% of the questions I receive. Questions about interesting aspects of LLM usage are usually positive and friendly, but environment setup questions often come from frustrated learners. Some students even feel like giving up when things don’t work as expected. I recall one student who was extremely frustrated because his OpenAI API key wouldn’t work. After much discussion, it turned out he had misspelled "OpenAI_API_key" as "open_API_key". This is a common trap, and while it may feel discouraging, these experiences are part of the learning process.

Environment setup can be tricky. Simple typos, overlooked hyphens, or architectural mismatches can cause hours of debugging. For example, I once deployed a binary from my Mac to AWS, and it failed because of system architecture differences. The error message referenced "pedantic", which seemed unrelated and misleading, causing a multi-day troubleshooting process. Everyone will encounter unique issues during setup, and a big part of learning is developing the skill to identify the root cause when error messages are confusing.

This brings me to the course’s terms of service, which I ask you to accept. There are five key commitments. First, embrace roadblocks with enthusiasm and a positive attitude. These challenges are frustrating, but they’re where real learning occurs, even for experienced professionals. Second, when you encounter problems, roll up your sleeves and dig in. Experiment, research, post on forums or Stack Overflow, and investigate the issue thoroughly, because solutions often require deep exploration. Use LLMs like ChatGPT or Claude to assist you by pasting stack traces or error messages, but remember that LLMs have limitations and blind spots. They often provide outdated model names or suggest surface-level fixes rather than addressing the root cause.

For example, during my Mac-to-AWS deployment issue, LLMs initially suggested workarounds for the pedantic error rather than recognizing that the root cause was an architecture mismatch. Only after carefully analyzing the situation did I identify the true problem. LLMs are excellent tools, but context is key. Always provide them with full context and emphasize that the goal is to find the underlying problem, not just patch the immediate error. Developing this habit of analyzing root causes and considering the bigger picture is essential for troubleshooting and problem-solving throughout the course.

# **G) Day 1 - Course Expectations and Community Support for Production AI**

The fourth term asks you to acknowledge that if you post issues on Udemy, I will always try to help. However, my assistance may not be as straightforward as with my other courses because this course involves complex environment setups. In my other courses, when students post problems, they are usually about code that I can reproduce and fix quickly. In this course, many issues relate to your environment, setup, and accounts, which I may not be able to replicate exactly. I can provide hints and guidance, but it may be harder for me to give direct step-by-step instructions. You should still post your questions, and I will do my best to help, but recognize that you may need to persist longer than before. If at any point you feel that a particular setup, such as Azure, is not worth the effort, you can switch to alternatives like GCP. You can always follow along with the videos to absorb the learning even if you hit a roadblock.

I also encourage you to actively engage with the Udemy Q&A. When you solve a problem, post your solution to help others, and look for questions from other students that you might be able to assist with. This is especially valuable in this course because students have different systems and may encounter unique issues. Supporting each other in the community can make a significant difference, and this is more critical here than in any of my previous courses. By helping others, you reinforce your own learning and create a collaborative environment where everyone benefits.

To formalize this, I am piloting a new feature in Udemy that allows more interaction between you and the course. I would like you to review these five terms carefully, and to confirm your agreement, I ask that you say aloud, “I agree.” This is a way of acknowledging your commitment to these terms. Even if you do not literally say it out loud, understanding and accepting the terms is what matters most. Thank you for agreeing to these terms—it is essential for the success of this course.

Finally, I want to remind you that I am available to help and connect with you throughout the course. You can reach me via Udemy Q&A, where I will always respond, though it may be more challenging to provide direct solutions due to the complexity of environment issues. You can also reach me by email or connect with me on LinkedIn. I welcome LinkedIn connections, and if you post your course projects and assignments there, tagging me allows me to provide feedback and amplify your work. Sharing your progress on LinkedIn can showcase your expertise to a wider audience, including potential future clients or employers.

As we wrap up day one, let’s revisit the curriculum. With the first day highlighted as complete, you have successfully deployed your first simple AI app. This marks 5% completion of your journey toward deploying generative and agentic AI into production. Congratulations on reaching this milestone! There is much more ahead, and we begin with day two tomorrow. I am excited to continue this journey with you and can’t wait to see the projects you will build in the coming days.

# **H) Day 2 - Building Full-Stack AI Apps: Frontend-Backend Architecture for LLMs**

Well, if you're still here, that means you haven't been completely put off by day one, which was quite a marathon day. And you're here back for day two of Genii and Agentic AI in production. So what do I have in store for you today? We're going to be talking about the front end and the back end, putting them together to make a full stack application. And I gotta tell you, we are hardly going to scrape the surface of what it takes to write a front end. I imagine some of you are actually front end engineers on this course, looking to get more well-rounded. And for you, the next few slides are going to be kind of easy. For some of you, you might be completely new to front end engineering. I'm just going to cover the basics to give you a framework, to give you the intuition for what it is that goes on when you build the front end side of a web application. I'm going to ask you to review the code, to look at what I'm doing, and get an LLM to explain it if you don't understand it. They're also amazing at generating front end code. Use this to give you the foundation of what's going on when you build a front end and a back end to work together.

Just to start with some definitions: obviously, when we say front end, we're referring to the code that runs in the user's browser—a combination of HTML for the structure of the page, CSS for the appearance of the page, and JavaScript, which is used, for example, for what happens when a button is clicked or for making animations move. It's the interactivity of the page. The back end, on the other hand, is the business logic running on the server. It includes database access, calling LLMs, potentially calling other APIs, storing your secrets, and the like. Your typical web application, of course, has a front end and a back end. The front end makes API calls to the back end by hitting a particular URL, which we call a web endpoint. It hits that URL, which calls the backend code. The backend generates some content and returns it to the front end, typically in the form of a JSON object, and the front end collects that JSON object and uses it to make some changes to the UI.

There is another way the back end is sometimes used, which is that it can be called to create an entire web page, serving up a whole web page to the front end. So the front end might call my home page, and when that URL is called, the server generates the entire home page front end code and passes that back to the browser. So that's a second way that server back end code is used, and we'll be doing both of them in our labs. Now, you may be wondering how Gradio fits into this. If you've taken my other courses, you'll know that I'm a huge fan of Gradio. I use it a lot to build quick UIs that look great, in my opinion. You might be wondering, "Where does the front end/back end of Gradio come in?" Well, here's the answer. Gradio is a back end framework, but confusingly, Gradio is able to generate front end code, and that's kind of kept behind the scenes for us. We write and describe a UI with Python, and Gradio generates the front end for us when it's serving up that user interface. That's why when you're using Gradio, it's a kind of front end/back end in one—because the back end code is able to generate the front end code. We won't be using Gradio in this course because we're doing stuff for reals; we're going to be building real front ends and back ends.

I just want to talk about the progression of different technologies that have happened over time when it comes to front end code. The front end landscape started off some time ago with vanilla HTML web pages, and still some people do this. There's something very satisfying about writing this way. This is where you just have basic HTML code to describe a web page, write your own CSS to do the styling, and have JavaScript. Typically, people use lightweight, low-level libraries like jQuery, which simplify some of the things you have to write in JavaScript. A lot of what you're doing when you write JavaScript at this level is manipulating something called the DOM, which stands for Document Object Model. This is the JavaScript model that represents the web page being rendered in the browser, so that you can do things like navigate around in the code to find a button and then change its color with JavaScript code. That model of what the web page looks like on the browser’s screen is known as the DOM.

Next came the JavaScript front end frameworks—an abstraction layer to make it quicker and more reusable to build sophisticated web pages. There’s a zoo of these frameworks, the most popular by far being React. There's also Vue, Angular, and Svelte, among others. They are all about building UIs from reusable components that you can plug and play together to construct quite sophisticated UIs quickly. Most of the time when you're working with a framework like React, you're building what's known as a single page application (SPA). This refers to the fact that, even though as a user it feels like you're clicking around lots of different web pages, the whole React application—the content and front end code—is loaded up once when you first hit the first URL. From that point onwards, when you're clicking around the application, you're typically making a series of API calls to the server, but it’s all running effectively on the same web page, even though it appears to redraw itself in different ways and feels like you're navigating around.

React and the other frameworks come in two flavors: a JavaScript variant, which is what it began with, and a more modern TypeScript variant, which is React using TypeScript, a strongly typed version of JavaScript. If you're not sure what that means, ask ChatGPT—it will explain it brilliantly. The final step in this progression is the application frameworks that came next and are the most recent part of this story, such as Next.js, which we will be using. React is super flexible and quite bare bones; it doesn’t come with a lot of features like form validation or routing between different sections of your app. The assumption is that if you're using React, you pick and choose which frameworks you want to add together. This is great for flexibility but means there are many different ways to build React apps. Next.js is a higher-level framework built on top of React that bundles together many useful tools so that you don't need to make lots of different decisions—you can just use Next.js and go. It has routing and data fetching built-in, supports server-side rendering, and comes with a bunch of tools built-in, making it very quick to get started.

This completes the narrative of the three tiers of technology that have formed modern front end development.

# **I) Day 2 - Building Full-Stack AI Apps with React, FastAPI, and NextJS**



# **J) Day 2 - Building Your First Full-Stack AI SaaS with NextJS and FastAPI**

# **K) Day 2 - Building Your First FastAPI Backend for Production LLM Deployment**

# **L) Day 2 - Deploying Full-Stack AI Apps with Next.js Frontend and FastAPI Backend**

# **M) Day 2 - Adding Real-Time Streaming and Professional UI to Your LLM App**



